{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e24572e",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868fd94",
   "metadata": {},
   "source": [
    "1. 为什么BERT在第一句前会加一个[CLS]标志?\n",
    "- 下游任务fine-tuning时，需要这个标志位用作输入，尤其是分类任务\n",
    "2. BERT的三个Embedding直接相加会对语义有影响吗？\n",
    "- 不会；最本质的还是在于，由于embedding维度高（768），所以即使把3个组分直接相加，还是能够被模型所区分（参考：https://www.zhihu.com/question/374835153 ）\n",
    "3. 在BERT中，token分3种情况做mask，分别的作用是什么？\n",
    "- 第一种：Encoder中的Mask，作用是作为一个预训练任务MLM\n",
    "- 第二种：Decoder中的Mask，作用是防止解码时看到未来step的信息\n",
    "- 第三种：Padding Mask，为了处理非定长序列，需要做padding，在进行attention时，把这些padding的无意义位置置为负无穷（点积操作之后），保证经过softmax后值为0（参考：https://www.cnblogs.com/wevolf/p/12484972.html ）\n",
    "4. 为什么BERT选择mask掉15%这个比例的词，可以是其他的比例吗？\n",
    "- 应该是作者尝试之后觉得这个数字效果最佳；可以，这是一个超参\n",
    "- 对于每个输入的sequence，随机选取15%的tokens进行mask（代之以[MASK]符号），被mask的token对应的输出后接softmax，预测该位置的真实token（类比其它LM的做法）。由于fine-tuning过程不存在[MASK]符号，为了克服这个缺点，论文中采用了这样的技巧：对于被选中需mask的token，80%的概率替换为[MASK]，10%的概率替换为另一个随机token，10%的概率不作任何替换\n",
    "5. 针对句子语义相似度/多标签分类/机器翻译翻译/文本生成的任务，利用BERT结构怎么做fine-tuning？\n",
    "- 用[SEP]拼接，对[CLS]做分类\n",
    "- 多标签：变成多个分类问题，[CLS]后接多个MLP，做多次二分类\n",
    "- 机器翻译：BERT作为Encoder\n",
    "- 文本生成：参考MASS、UNILM的结构\n",
    "6. 使用BERT预训练模型为什么最多只能输入512个词，最多只能两个句子合成一句？\n",
    "- 因为seq_len是固定的\n",
    "7. BERT非线性的来源在哪里？multi head attention 是线性的嘛？\n",
    "- 激活函数（gelu）、attention（来自softmax）\n",
    "- 不是\n",
    "8. BERT 是如何区分一词多义的？\n",
    "- BERT的Encoder是需要考虑上下文的，一词多义通过上下文来区分\n",
    "9. BERT的输入是什么，哪些是必须的，为什么position id不用给，type_id 和 attention_mask没有给定的时候，默认会是什么\n",
    "- word embedding、positional encoding、segment embedding；前二者是必须的\n",
    "10. BERT训练时使用的学习率 warm-up 策略是怎样的？为什么要这么做？\n",
    "- 在训练最初使用较小的学习率来启动，并很快切换到大学习率而后进行常见的 decay\n",
    "- 具体原因目前还没有很好地证明\n",
    "11. Bert 采用哪种Normalization结构，LayerNorm和BatchNorm区别，LayerNorm结构有参数吗，参数的作用？\n",
    "- Layer Normalization\n",
    "- 归一化的维度不同，前者在batch维度，依赖多个样本，后者在layer维度，仅依靠一个样本\n",
    "- 有两个超参，用于学习出归一化前的数据分布，避免模型的表达能力因为归一化而下降\n",
    "12. 为什么说ELMO是伪双向，BERT是真双向？产生这种差异的原因是什么？\n",
    "- ELMo的做法是，将一个前向的语言模型和一个后向的语言模型进行拼接；BERT的做法是，用一个语言模型，但是同时考虑上下文，更加自然\n",
    "- 是它们的结构不同\n",
    "13. BERT和Transformer Encoder的差异有哪些？做出这些差异化的目的是什么？\n",
    "- positional encoding：前者是随机初始化，后者用三角函数；不清楚\n",
    "- segment embedding：只有前者有\n",
    "- CLS、SEP：只有前者才有；用于下游任务的fine-tuning\n",
    "14. BERT训练过程中的损失函数是什么？\n",
    "- 交叉熵（两个与训练任务的loss之和）\n",
    "15. BERT 的两个任务 Masked LM 任务和 Next Sentence Prediction 任务是先后训练的还是交替训练的\n",
    "- 同时训练\n",
    "16. 如何优化BERT性能\n",
    "- 层数缩减\n",
    "- 蒸馏\n",
    "17. 在BERT应用中，如何解决长文本问题？\n",
    "- 一种是截取：截取前510个或后510个或前128+后382\n",
    "- 一种是分段k=L/510,然后各段可以求平均、求max、或者加个attention融合\n",
    "- 用XLNet，该模型基于Transformer-XL，后者实现了片段级的递归机制（类似LSTM）\n",
    "18. word2vec和BERT的异同\n",
    "- BERT的思想和word2vec的CBOW很像，但是BERT的模型更复杂，预料更大，且词向量是动态的，可以解决一词多义的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7acadfa",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f9f0d",
   "metadata": {},
   "source": [
    "1. GPT2和GPT的区别?\n",
    "- GPT-2去掉了fine-tuning层：不再针对不同任务分别进行微调建模，而是不定义这个模型应该做什么任务，模型会自动识别出来需要做什么任务（prompt）\n",
    "- 提升训练数据的数量、质量、广泛度\n",
    "- 模型结构在GPT的基础上做了简单调整：LN移至每个sub-block的输入位置、最后一个self-attention block后增加一个LN、调整部分参数的初始化方式、扩大词表、seq_len从512提升至1024、batchsize采用512\n",
    "- 备注：pre layer norm比post layer norm效果好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10c428",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4bd389",
   "metadata": {},
   "source": [
    "1. Transformer在哪里做了权重共享，为什么可以做权重共享？好处是什么\n",
    "- Encoder的embedding和Decoder的embedding、Decoder的embedding和Decoder的softmax\n",
    "- 切词方式是bpe，最小单位是subword，如果源语言和目标语言有很多相同的subword，这种做法可以共享语义；如果源语言和目标语言相差较大，可能共享语义的作用也很小；另外，共用词表会导致词表数量增大，增加softmax的计算时间，因此实际使用中是否共享可能要根据情况权衡\n",
    "- 这样一来，softmax的每一个输出，就相当于decoder输出向量和每一个embedding的向量内积，代表了当前output和所有词向量的距离，或者说相关程度，可以提升softmax的表现\n",
    "- 减少参数量，防止过拟合\n",
    "2. Transformer的点积模型做缩放的原因是什么？\n",
    "- 对于一个向量，随着向量各元素值数量级的增加，softmax之后，它的概率会倾向于全部分配给值最大的元素，这样会导致softmax函数的梯度非常小，不利于反向传播；之所以用维度的根号来缩放，是因为这样可以使得点积数据的元素值的方差变成1（参考：https://www.zhihu.com/question/339723385 ）\n",
    "3. Transformer中是怎么做multi head attention 的，这样做multi head attention，会增加它的时间复杂度嘛？\n",
    "- 采用多个低维度的head，对特征空间进行切割\n",
    "- 不会\n",
    "4. 为什么Transformer 要做 Multi-head Attention? 它的好处在哪？\n",
    "- 从不同子空间去捕获特征，提升效果\n",
    "5. Transformer的Encoder端和Decoder端是如何进行交互的？和一般的seq2seq有什么差别\n",
    "- Encoder最后一个layer的输出将转换为K、V，作为Decoder中每一个encoder-decoder attention的输入\n",
    "6. Transformer中multi-head attention中每个head为什么要进行降维？\n",
    "- 高维空间学习难度大，所以对其进行切割\n",
    "7. 为何在获取输入词向量之后需要对矩阵乘以embeddding size的开方\n",
    "- embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛\n",
    "8. 不考虑多头的原因，self-attention中词向量不乘QKV参数矩阵，会有什么问题？\n",
    "- 如果不乘QKV矩阵，那么q=k=v，在相同量级的情况下，qi与ki点积的值会是最大的，在softmax后的加权平均中，该词本身所占的比重将会是最大的（会是一个对称矩阵），使得其他词的比重很少，无法有效利用上下文信息来增强当前词的语义表示；而乘以QKV参数矩阵，会使得每个词的q,k,v都不一样，能很大程度上减轻上述的影响\n",
    "- 提升泛化能力\n",
    "9. Self-Attention 的时间复杂度是怎么计算的？\n",
    "- O(n^2 * d)，n是序列长度，d是embedding size\n",
    "10. 位置编码技术\n",
    "- 绝对位置编码：训练式（随机初始化，如BERT、GPT）、三角函数式（Transformer，不会出现OOV，再长的序列也可以计算出位置编码）\n",
    "- 相对位置编码：Transformer-XL（直接修改attention score的矩阵，而不是加到embedding上）\n",
    "- 参考：https://kexue.fm/archives/8130\n",
    "11. Transformer为什么要用LN而不是BN？\n",
    "- 用BN效果不好，可能是因为对于自然语言来说，样本维度内的信息进行归一化更有效，而batch内的统计量不稳定，效果不好\n",
    "- 参考：https://github.com/DA-southampton/NLP_ability/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E7%AD%94%E6%A1%88%E5%90%88%E8%BE%91.md\n",
    "12. 采用了什么激活函数？（前馈层）\n",
    "- ReLU\n",
    "13. Decoder端可以并行化么？\n",
    "- 训练的时候可以，inference的时候不可以\n",
    "14. FFN的作用\n",
    "- Transformer在抛弃了 LSTM 结构后，FFN 中的 ReLU成为了一个主要的提供非线性变换的单元。\n",
    "15. Transformer的绝对位置编码中，包含相对位置信息吗?\n",
    "- 本来包括，但是经过self-attention后，相对位置信息消除了\n",
    "16. Decoder有哪些解码方式\n",
    "- greedy search：每次取概率最大的\n",
    "- beam search：每次取概率最大的前K个，例如，生成第一个词的时候，取概率最大的前K个，生成第二个词的时候，对第一个词的K种选择分别尝试，然后对生成的所有第二个词里，取概率最大的前K个(取第一个词概率乘以第二个词概率乘积最大的，下同)\n",
    "- 参考：https://www.youtube.com/watch?v=RLWuzLLSIgw\n",
    "- 随机采样：按输出概率进行随机采样，或者在概率最高的topK中进行采样（对topK进行概率归一化）\n",
    "17. seq2seq，训练的时候，Decoder是如何操作的？\n",
    "- 采用teacher-forcing技术，每一个时间步的输入都是ground truth，好处就是可以让模型并行计算（Transformer），在训练的时候矫正模型的预测，避免在序列生成的过程中误差进一步放大，极大的加快模型的收敛速度，令模型训练过程更加快&平稳\n",
    "- 参考：https://zhuanlan.zhihu.com/p/93030328\n",
    "18. Transformer中使用最多的层？\n",
    "- dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f6846",
   "metadata": {},
   "source": [
    "# XLNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54c9e8",
   "metadata": {},
   "source": [
    "1. XLNet如何实现在不加 [Mask] 情况下利用上下文信息呢？\n",
    "- 通过PLM任务，对于一个输入，采样它的全排列，然后利用AR的方式去训练，这样，对于一个固定位置的token，它就有可能同时看到它的上下文信息\n",
    "2. XLNet为什么要用双流注意力？两个流的差别是什么？分别的作用是什么？分别的初始向量是什么？\n",
    "- query stream：只能看到当前位置之前的信息，以及当前位置的位置信息；content stream：能看到当前位置及之前的信息（位置+内容）\n",
    "- 如果预测的位置是t，那么只能看到t的位置信息，而不能有t的内容，但是其它的位置（t的上文）却需要有位置+内容的信息\n",
    "- query stream的初始化是随机的，content stream的初始化是embedding\n",
    "- 参考：https://zhuanlan.zhihu.com/p/107560878 、https://blog.csdn.net/u012526436/article/details/93196139\n",
    "3. 虽然不需要改变输入文本的顺序，但 XLNet 通过 PLM 采样输入文本的不同排列去学习，这样不会打乱或者丢失词汇的时序信息嘛？\n",
    "- 不会，因为token的时序信息包含在position encoding中，而PLM只是改变了token的上文，而对应上文的embedding所包含的position encoding没有变化\n",
    "4. AutoRegressive (AR) language modeling and AutoEncoding (AE) 两种模式分别是怎样的，各自的优缺点是什么，XLNet又是怎样融合这两者的？\n",
    "- AR：从左到右，或者从右到左，只利用单向的信息；AE，同时利用双向信息\n",
    "- AR缺点：只能利用单向信息；AE缺点：MASK符号在fine-tuning阶段并不存在，被MASK的词是独立的\n",
    "- 通过PLM任务，既可以利用上下文，又可以不新增MASK符号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054151fb",
   "metadata": {},
   "source": [
    "# ALBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9d132",
   "metadata": {},
   "source": [
    "1. ALBERT的小具体小在哪里？对于实际存储和推理都有帮助嘛？\n",
    "- 将embedding的大小E和隐层的大小H解耦，并在embedding后接一个E*H的转换矩阵，降低参数量\n",
    "- Encoder中每一个layer的参数共享\n",
    "- 对实际存储有帮助，因为降低了参数量，这样也可以加速训练（收敛快）；对推理无帮助，因为网络结构大小没变\n",
    "2. BERT的NSP为什么被认为没有效？ALBERT采样的SOP（Sentence Order Prediction）任务是怎样的？相比NSP有什么优势？\n",
    "- 任务太简单了\n",
    "- 两个连续的句子作为正样本，调换顺序后作为负样本\n",
    "- 任务更困难"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde31979",
   "metadata": {},
   "source": [
    "# self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1610593",
   "metadata": {},
   "source": [
    "1. 深度学习中Attention与全连接层的区别何在？\n",
    "- 全连接层中的权重，对于所有的样本，都是固定的、相同的；attention模式下的权重，对于不同的样本，则是不同的，而且它利用输入的特征信息来确定哪些部分更重要\n",
    "2. self-attention 的本质是什么？包括哪几个步骤？和普通 Attention 的差别在哪里？\n",
    "- 本质就是自己和自己进行attention，用文本中的其它词来增强目标词的语义表示，从而更好的利用上下文的信息\n",
    "- 点积、softmax、加权\n",
    "- 普通attention是k=v，这里有q=k=v\n",
    "3. 在普通 attention 中，一般有 k=v，那 self-attention 可以嘛？\n",
    "- 也可以，self-attention中，本来是q=k=v，在经过head矩阵的变换后，三者均不相等了，当然也可以令k=v\n",
    "4. self-attention 在计算的过程中，如何对padding位做mask？\n",
    "- 点积之后，相应的位置置为负无穷\n",
    "5. bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？\n",
    "- 简单来说，如果用attention mask，这种情况下被mask的部分就只有一个position id，想要预测出被mask的词，是非常困难的，有了[mask]，可以作为一个占位符（https://www.zhihu.com/question/318355038 ）\n",
    "6. XLNet为什么不直接在attention掩码矩阵中只把当前的单词掩盖住来获取上下文的信息呢？直接mask住左上到右下的对角线构建双向语言模型不行吗？\n",
    "- 不行，这样会泄漏下文信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266802c",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e9e26",
   "metadata": {},
   "source": [
    "1. LSTM的参数量\n",
    "- 4 \\* (d_h \\* (d_h + d_x) + d_h)\n",
    "- 参考：https://zhuanlan.zhihu.com/p/147496732\n",
    "2. cell和h分别代表什么？\n",
    "- hidden state里存储的，主要是“近期记忆”；cell state里存储的，主要是“远期记忆”。cell state的存在，使得LSTM得以对长依赖进行很好地刻画\n",
    "- 参考：https://www.zhihu.com/question/68456751"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb553b53",
   "metadata": {},
   "source": [
    "# 其它"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bfadc",
   "metadata": {},
   "source": [
    "1. 输入用char粒度的好处？\n",
    "- 避免OOV\n",
    "- 词表更小\n",
    "- 不必受限于切词技术\n",
    "2. ReLU的缺点？\n",
    "- 梯度爆炸\n",
    "- dead ReLU：负半区取值为零，会导致梯度不更新\n",
    "- 输出的均值和方差不是0和1\n",
    "3. 如何减少训练好的神经网络模型的推理时间？\n",
    "- 在GPU/TPU/FPGA上进行服务\n",
    "- 16位量化，部署在支持fp16的GPU上提供服务\n",
    "- 剪枝以减少参数\n",
    "- 知识蒸馏(用于较小的transformer模型或简单的神经网络)\n",
    "- 层次化softmax\n",
    "- 你也可以缓存结果\n",
    "4. 什么是label smoothing（标签平滑）？\n",
    "- soft one-hot，类别为1的概率改成（1-\\epsilon），其它类别的概率改成（\\epsilon / (K - 1)），这样可以增加模型的泛化性能\n",
    "5. Multi-Task中，什么是硬参数共享、软参数共享？\n",
    "- 硬参数共享：多个任务之间共享网络的同几层隐藏层，只不过在网络的靠近输出部分开始分叉去做不同的任务\n",
    "- 软参数共享：不同的任务使用不同的网络，但是不同任务的网络参数，采用距离(L1,L2)等作为约束，鼓励参数相似化\n",
    "6. AdamW和Adam的区别？\n",
    "- 前者在后者的基础上加入了L2正则\n",
    "7. self-attention复杂度，LSTM复杂度？\n",
    "- O(n^2\\*d)；O(n\\*d^2)（参考：https://zhuanlan.zhihu.com/p/264749298 ）\n",
    "8. 残差连接的作用？\n",
    "- 避免梯度消失\n",
    "9. dropout的作用？\n",
    "- 防止过拟合\n",
    "10. BN的作用？（其它的归一化也一样）\n",
    "- 减轻对参数初始化的依赖，利于调参\n",
    "- 加速收敛\n",
    "- 一定程度上防止过拟合\n",
    "- 缓解梯度消失、梯度爆炸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f30086",
   "metadata": {},
   "source": [
    "# 语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb3c422",
   "metadata": {},
   "source": [
    "1. 什么是perplexity？它在NLP中的地位是什么？\n",
    "- 混淆度，是语言模型计算每一个词得到的概率倒数的几何平均，用来刻画语言模型的能力，混淆度越低，模型能力越强，混淆度取log便是交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e6fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
